{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laptop Stand: face detection and gaze orientation\n",
        "Alena, Siying, Kris\n",
        "\n",
        "## Overview\n",
        "tutorial: https://www.assemblyai.com/blog/mediapipe-for-dummies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import PyQt5\n",
        "from PIL import Image\n",
        "from IPython.display import Video\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_holistic = mp.solutions.holistic\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_face_mesh = mp.solutions.face_mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ref: https://unsplash.com/photos/smiling-woman-standing-while-holding-orange-folder-FcLyt7lW5wg\n",
        "img = Image.open('student_photo.jpg')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file='student_photo.jpg'\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "\n",
        "# Create a face mesh object\n",
        "with mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=True, # change to dynamic\n",
        "        max_num_faces=1,\n",
        "        refine_landmarks=True,\n",
        "        min_detection_confidence=0.5) as face_mesh:\n",
        "\n",
        "    image = cv2.imread(file)\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "face_found = bool(results.multi_face_landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if face_found:\n",
        "    annotated_image = image.copy()\n",
        "    \n",
        "    mp_drawing.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=results.multi_face_landmarks[0],\n",
        "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
        "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
        "    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1))\n",
        "        \n",
        "    # Save image\n",
        "    cv2.imwrite('face_tesselation_only.png', annotated_image)\n",
        "\n",
        "# Open image\n",
        "img = Image.open('face_tesselation_only.png')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the image filename\n",
        "file = 'student_photo.jpg'\n",
        "\n",
        "# Create a MediaPipe `Pose` object\n",
        "with mp_pose.Pose(static_image_mode=True, \n",
        "\t\t  model_complexity=2,\n",
        "                  enable_segmentation=True) as pose:\n",
        "        \n",
        "    image = cv2.imread(file)\n",
        "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "annotated_image = image.copy()\n",
        "\n",
        "mp_drawing.draw_landmarks(annotated_image, \n",
        "                          results.pose_landmarks, \n",
        "                          mp_pose.POSE_CONNECTIONS,\n",
        "                          landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
        "                          connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2))\n",
        "\n",
        "filename = \"pose_wireframe.png\"\n",
        "cv2.imwrite(filename, annotated_image)\n",
        "\n",
        "display(Image.open(filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UPPER_BODY_LANDMARKS = [\n",
        "    mp_holistic.PoseLandmark.nose_3d,\n",
        "    # mp_holistic.PoseLandmark.LEFT_EYE_INNER,\n",
        "    mp_holistic.PoseLandmark.LEFT_EYE,\n",
        "    # mp_holistic.PoseLandmark.LEFT_EYE_OUTER,\n",
        "    # mp_holistic.PoseLandmark.RIGHT_EYE_INNER,\n",
        "    mp_holistic.PoseLandmark.RIGHT_EYE,\n",
        "    # mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
        "    mp_holistic.PoseLandmark.LEFT_EAR,\n",
        "    mp_holistic.PoseLandmark.RIGHT_EAR,\n",
        "    mp_holistic.PoseLandmark.LEFT_SHOULDER,\n",
        "    mp_holistic.PoseLandmark.RIGHT_SHOULDER\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landmarks = results.pose_world_landmarks.landmark\n",
        "\n",
        "upper_body_data = {}\n",
        "for lm in UPPER_BODY_LANDMARKS:\n",
        "    name = lm.name  # e.g., 'LEFT_SHOULDER'\n",
        "    x, y, z = landmarks[lm.value].x, landmarks[lm.value].y, landmarks[lm.value].z\n",
        "    upper_body_data[name] = (x, y, z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upper_body_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = 'student_video.mp4'\n",
        "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    cap = cv2.VideoCapture(file)\n",
        "\t# Get the number of frames in the video\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # Create a NumPy array to store the pose data as before\n",
        "    # The shape is 3x33x144 - 3D XYZ data for 33 landmarks across 144 frames\n",
        "    data = np.empty((3, len(UPPER_BODY_LANDMARKS), length))    \n",
        "    \n",
        "\t# For each image in the video, extract the spatial pose data and save it in the appropriate spot in the `data` array \n",
        "    frame_num = 0\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image)\n",
        "        \n",
        "        landmarks = results.pose_world_landmarks.landmark\n",
        "        for i, lm_id in enumerate(UPPER_BODY_LANDMARKS):\n",
        "            data[:, i, frame_num] = (\n",
        "                landmarks[i].x,\n",
        "                landmarks[i].y,\n",
        "                landmarks[i].z\n",
        "            )  \n",
        "        \n",
        "        frame_num += 1\n",
        "    \n",
        "    # Close the video file\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(5, 5, True)\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "# Setup the 3D axes\n",
        "ax.set_xlim3d(-1, 1)\n",
        "ax.set_ylim3d(-1, 1)\n",
        "ax.set_zlim3d(-1, 1)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "# Initial frame\n",
        "x = data[0, :, 0]\n",
        "y = data[1, :, 0]\n",
        "z = data[2, :, 0]\n",
        "points, = ax.plot(x, y, z, 'o-', color='blue')\n",
        "\n",
        "# Define an update function for each frame\n",
        "def update(frame):\n",
        "    points.set_data(data[0, :, frame], data[1, :, frame])\n",
        "    points.set_3d_properties(data[2, :, frame])\n",
        "    return points,\n",
        "\n",
        "# Create the animation\n",
        "anim = FuncAnimation(fig, update, frames=data.shape[2],\n",
        "                     interval=33, blit=True)\n",
        "\n",
        "from matplotlib.animation import FFMpegWriter\n",
        "\n",
        "# Set up the writer explicitly\n",
        "writer = FFMpegWriter(fps=30, codec='libx264', extra_args=['-pix_fmt', 'yuv420p'])\n",
        "\n",
        "# Save the animation\n",
        "anim.save('walking_wireframe.mp4', writer=writer, dpi=300) # note : might bneed to ru n brew install ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def landmarks_to_control(landmarks, k=0.5, target=(0.0, 0.2, -0.3)):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    -   landmarks: 3d coord of body parts\n",
        "    -   k: proportion of adjustment; k in (0, 1]\n",
        "    -   target_pos: ideal location of 3d_nose\n",
        "        Default centered nose position:\n",
        "        x = 0.0\n",
        "        y = 0.2             # slightly above body's COM\n",
        "        z = -0.3            # slightly in front of the body's COM\n",
        "\n",
        "    \n",
        "    Returns:\n",
        "    adjustment in 3D world coordinates\n",
        "    \"\"\"\n",
        "\n",
        "    nose_3d = np.array(landmarks['NOSE'])\n",
        "    eye_mid_3d = (np.array(landmarks['LEFT_EYE']) + np.array(landmarks['RIGHT_EYE'])) / 2.0\n",
        "    left_sh_3d = np.array(landmarks['LEFT_SHOULDER'])\n",
        "    right_sh_3d = np.array(landmarks['RIGHT_SHOULDER'])\n",
        "    shoulder_mid_3d = (left_sh_3d + right_sh_3d) / 2.0\n",
        "    \n",
        "    # body vector (torso -> head)\n",
        "    #     o|o               o|o\n",
        "    #    _/_                _|_\n",
        "    #    /          ->       |\n",
        "    #   =                    =\n",
        "    #  ||                   ||\n",
        "    view_vector_3d = (nose_3d + eye_mid_3d) / 2.0 - shoulder_mid_3d\n",
        "    view_vector_3d /= np.linalg.norm(view_vector_3d)\n",
        "    \n",
        "    # offsets rel to laptop position at origin, assume screen centers at (0,0,0)\n",
        "    horizontal_error = nose_3d[0] # 3d coodinates\n",
        "    vertical_error   = nose_3d[1]\n",
        "    depth_error      = nose_3d[2]  # negative = farther\n",
        "\n",
        "    pitch = np.arctan2(view_vector_3d[1], view_vector_3d[2])\n",
        "    yaw   = np.arctan2(view_vector_3d[0], view_vector_3d[2])\n",
        "    \n",
        "    # control command ~ adjustments\n",
        "    adjustments = {\n",
        "        'dx': -k * horizontal_error,\n",
        "        'dy': -k * vertical_error,\n",
        "        'dz': -k * depth_error,        # optional if stand can move forward/back\n",
        "        'tilt_pitch': -np.degrees(pitch),  # negative to align screen with view\n",
        "        'tilt_yaw': -np.degrees(yaw)\n",
        "    }\n",
        "    \n",
        "    return adjustments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PoseLandmark = mp.solutions.pose.PoseLandmark\n",
        "POSE_TO_I = {\n",
        "\t'NOSE': PoseLandmark.NOSE.value,\n",
        "\t'LEFT_SHOULDER': PoseLandmark.LEFT_SHOULDER.value,\n",
        "\t'RIGHT_SHOULDER': PoseLandmark.RIGHT_SHOULDER.value,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw adj\n",
        "def project_to_image(point_3d, scale, img_center_2d):\n",
        "    \"\"\"\n",
        "    Simple orthographic projection to 2D image space\n",
        "    scale: pixels per meter\n",
        "    img_center_2d: center of the image\n",
        "    \"\"\"\n",
        "    if not img_center_2d or not scale:\n",
        "        print(\"no img center or scale given\")\n",
        "        return None\n",
        "    x = int(img_center_2d[0] + point_3d[0] * scale)\n",
        "    y = int(img_center_2d[1] - point_3d[1] * scale)  # y-axis inverted for image\n",
        "    return (x, y)\n",
        "\n",
        "def get_frame_params(results, h, w, left_sh_3d, right_sh_3d):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - results: MediaPipe model\n",
        "\n",
        "    Estimates body's COM as a middle between shoulders. It's techincally mid of hips, but we don't capture them in the photo.\n",
        "    Estimates scale (#pixels per meter): ratio of distance between shoulders in pixels vs in 3d coordinates\n",
        "    \"\"\"\n",
        "    left_shoulder_2d = results.pose_landmarks.landmark[POSE_TO_I['LEFT_SHOULDER']] # in img frame\n",
        "    right_shoulder_2d = results.pose_landmarks.landmark[POSE_TO_I['RIGHT_SHOULDER']]\n",
        "    shoulder_mid_2d = ((left_shoulder_2d.x + right_shoulder_2d.x) / 2, (left_shoulder_2d.y + right_shoulder_2d.y) / 2)\n",
        "    img_center_2d = shoulder_mid_2d\n",
        "\n",
        "    left_sh_px = np.array([left_shoulder_2d.x * w, left_shoulder_2d.y * h])\n",
        "    right_sh_px = np.array([right_shoulder_2d.x * w, right_shoulder_2d.y * h])\n",
        "    pixel_distance = np.linalg.norm(left_sh_px - right_sh_px) # image frame\n",
        "    real_distance = np.linalg.norm(left_sh_3d - right_sh_3d) # 3d world frame\n",
        "    scale = pixel_distance / real_distance # pixels per meter\n",
        "\n",
        "    return img_center_2d, scale\n",
        "\n",
        "# example:\n",
        "file = 'student_photo.jpg'\n",
        "\n",
        "# Create a MediaPipe `Pose` object\n",
        "with mp_pose.Pose(static_image_mode=True, model_complexity=2,\n",
        "                  enable_segmentation=True) as pose:\n",
        "    upper_body_data_3d = {} # lm name -> 3d-coord tuple\n",
        "\n",
        "    image = cv2.imread(file)\n",
        "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    world_landmarks = results.pose_world_landmarks.landmark\n",
        "   \n",
        "    for lm in UPPER_BODY_LANDMARKS:\n",
        "        i = lm.value # gives index in landmarks\n",
        "        x, y, z = world_landmarks[i].x, world_landmarks[i].y, world_landmarks[i].z\n",
        "        upper_body_data_3d[lm.name] = (x, y, z) # in 3d coord\n",
        "\n",
        "print(\"Upper body data in 3d: \", upper_body_data_3d)\n",
        "adjustments_3d = landmarks_to_control(upper_body_data_3d, k=0.7) # 3d coord\n",
        "print(\"Adjustment: \", adjustments_3d)\n",
        "\n",
        "nose_3d = np.array(upper_body_data_3d['NOSE'])\n",
        "left_sh_3d = np.array(upper_body_data_3d['LEFT_SHOULDER'])\n",
        "right_sh_3d = np.array(upper_body_data_3d['RIGHT_SHOULDER'])\n",
        "shoulder_mid_3d = (left_sh_3d + right_sh_3d) / 2.0\n",
        "\n",
        "h, w, _ = image.shape\n",
        "img_center_2d, scale = get_frame_params(results, h, w, left_sh_3d, right_sh_3d)\n",
        "\n",
        "view_vector_3d = nose_3d - shoulder_mid_3d # in 3d\n",
        "print(\"View vector: \", view_vector_3d)\n",
        "nose_2d = project_to_image(nose_3d, scale, img_center_2d) # 2d projection; we can also get in directly as above\n",
        "shoulder_2d = project_to_image(shoulder_mid_3d, scale, img_center_2d)\n",
        "\n",
        "# draw pose and adjustment vector\n",
        "annotated_image = image.copy()\n",
        "cv2.circle(annotated_image, center=nose_2d, radius=50, color=(0, 0, 255), thickness=2)\n",
        "cv2.circle(annotated_image, center=shoulder_2d, radius=50, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "# adjustment vector\n",
        "cv2.arrowedLine(annotated_image, shoulder_2d, nose_2d, (255, 0, 0), 2, tipLength=0.2)\n",
        "\n",
        "filename = \"pose_student.png\"\n",
        "cv2.imwrite(filename, annotated_image)\n",
        "cv2.destroyAllWindows()\n",
        "display(Image.open(filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def activate_actuator_speeds(control_signal_x, control_signal_y, control_signal_pitch, control_signal_yaw):\n",
        "\treturn # to do\n",
        "\n",
        "class PID:\n",
        "\tdef __init__(self, Kp, Ki, Kd, target=0.0):\n",
        "\t\tself.Kp = Kp\n",
        "\t\tself.Ki = Ki\n",
        "\t\tself.Kd = Kd\n",
        "\t\tself.target = target\n",
        "\t\tself.integral = 0.0\n",
        "\t\tself.prev_error = 0.0\n",
        "\n",
        "\tdef update(self, observed, dt):\n",
        "\t\terror = self.target - observed\n",
        "\t\tself.integral += error * dt\n",
        "\t\tif dt > 0:\n",
        "\t\t\t\tderivative = (error - self.prev_error) / dt\n",
        "\t\telse:\n",
        "\t\t\t\tderivative = 0\n",
        "\t\tself.prev_error = error\n",
        "\n",
        "\t\tcontrol_signal = (self.Kp * error) + (self.Ki * self.integral) + (self.Kd * derivative)\n",
        "\t\treturn control_signal\n",
        "\n",
        "pid_x = PID(1.0, 0.01, 0.1)\n",
        "pid_y = PID(1.0, 0.01, 0.1)\n",
        "pid_pitch = PID(2.0, 0.02, 0.2)\n",
        "pid_yaw = PID(2.0, 0.02, 0.2)\n",
        "\n",
        "# control cycle loop\n",
        "dt = 0.01  # 10ms\n",
        "while True:\n",
        "\tadjustments = landmarks_to_control(landmarks, k=1, target) # 3D world coordina; k=1 as step size handled in PID\n",
        "\tx_observed = adjustments.dx\n",
        "\ty_observed = adjustments.dy\n",
        "\t# z_observed = adjustments.dz ??\n",
        "\tpitch_observed = adjustments.tilt_pitch\n",
        "\tyaw_observed = adjustments.tilf_yaw\n",
        "\t\n",
        "\tcontrol_signal_x = pid_x.update(x_observed, dt)\n",
        "\tcontrol_signal_y = pid_y.update(y_observed, dt)\n",
        "\tcontrol_signal_pitch = pid_pitch.update(pitch_observed, dt)\n",
        "\tcontrol_signal_yaw = pid_yaw.update(yaw_observed, dt)\n",
        "\t\n",
        "\tactivate_actuator_speeds(control_signal_x, control_signal_y, control_signal_pitch, control_signal_yaw) # TODO\n",
        "\ttime.sleep(dt)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
